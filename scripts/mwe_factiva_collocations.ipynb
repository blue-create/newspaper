{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f8cef14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check working directory\n",
    "import os\n",
    "os.getcwd() # if directory is subfolder, change to home\n",
    "os.chdir('/home/sukayna/Documents/github/newspaper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c07cf9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import usual packages\n",
    "import json\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "import datetime as dt\n",
    "import locale\n",
    "from modules.preprocess_articles import *\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d36a8",
   "metadata": {},
   "source": [
    "### Load dataset + check frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57a6ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# always use json instead of pickle for persistence\n",
    "# to load the corpus\n",
    "with open('data/factiva_data.json', 'r') as f:\n",
    "    factiva_corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d92d4a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to get frequencies from corpus\n",
    "def Get_Frequency(mylist):\n",
    "    count = {}\n",
    "    for i in mylist:\n",
    "        count[i] = count.get(i, 0) + 1\n",
    "        count = {key: value for key, value in sorted(\n",
    "            count.items(), key = lambda item: item[1], reverse=True)}\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca749c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember corpus structure: title, body, date, newspaper\n",
    "\n",
    "# frequency of dates and newspapers in factiva corpus\n",
    "Get_Frequency(factiva_corpus[2]) # for dates\n",
    "Get_Frequency(factiva_corpus[3]) # for newspapers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be5871",
   "metadata": {},
   "source": [
    "### Format dates correctly\n",
    "Note: this can also be done in the dataframe cleaning step before preparing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "233fbe77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de_DE.utf8'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before pre-processing, change date format\n",
    "locale.setlocale(locale.LC_ALL, 'de_DE.utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ab4e6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, convert string dates to datetime format\n",
    "#factiva_corpus[2] # string date stored in a list\n",
    "\n",
    "# convert strings to datetime using `datetime` library\n",
    "factiva_corpus[2] = [dt.datetime.strptime(d, \"%d %B %Y\") for d in factiva_corpus[2]]\n",
    "\n",
    "# Year-Month-Day HH:MM:SS is the default output for string dates\n",
    "# keep date only from datetime object\n",
    "factiva_corpus[2] = [d.date() for d in factiva_corpus[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "82160728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-05\n"
     ]
    }
   ],
   "source": [
    "# print one element from list to check conversion\n",
    "print(factiva_corpus[2][1].isoformat()) \n",
    "# this is the correct format: %Y-%b-%d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea7d125",
   "metadata": {},
   "source": [
    "### Collocation analysis for major newspapers over time\n",
    "- preprocess corpus - remove punct + symobols + stop words only\n",
    "- find collocations across documents AND across years\n",
    "- calculate scores (rawfreq + PMI + chisq?) for comparison\n",
    "- plot comparisons over time: association strength (dot chart) / netowrk graphs / biplots (using semantic similarity)\n",
    "- check for collocation strength\n",
    "- significance testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35b0da47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f383dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "spacy_mod = spacy.load(\"de_core_news_lg\",\n",
    "                 disable=['ner', 'parser', 'tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93fefd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# may be necessary to examine subset only if process is slow\n",
    "# for i, item in enumerate(factiva_corpus):\n",
    "#    factiva_corpus[i] = item[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c3ee8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase corpus text\n",
    "# you should include this step in 'preprocess_articles.py' \n",
    "factiva_corpus[1] = [x.lower() for x in factiva_corpus[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9211bef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 4/4 [03:53<00:00, 58.43s/it]\n"
     ]
    }
   ],
   "source": [
    "### this section to be replaced by updated functions ###\n",
    "\n",
    "# run preprocessor on corpus\n",
    "for doc in tqdm(factiva_corpus):\n",
    "    newspaper_preprocessor = Preprocessor(newspaper_data = factiva_corpus, nlp = spacy_mod)\n",
    "    newspaper_preprocessor.tokenize()\n",
    "    newspaper_preprocessor.preprocess()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1127a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned corpus\n",
    "factiva_cleaned = newspaper_preprocessor.return_preprocessed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "72f7f549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stark', 'blutend', 'Schnittverletzung', 'alt', 'Mann']\n",
      "2564\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(factiva_cleaned[5][0:5], len(factiva_cleaned), type(factiva_cleaned), \n",
    "      sep='\\n')\n",
    "# tokens stored in list of docs \n",
    "# no metadata yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3de6539",
   "metadata": {},
   "source": [
    "### Collocations across all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "536fafa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stark', 'blutend', 'schnittverletzung', 'alt', 'mann']\n"
     ]
    }
   ],
   "source": [
    "# lowercase corpus text (again)\n",
    "# the class preprocesser breaks lowercasing :/\n",
    "# you should include this step in 'preprocess_articles.py' \n",
    "\n",
    "factiva_cleaned = [[word.lower() for word in doc] for doc in factiva_cleaned]\n",
    "print(factiva_cleaned[5][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ea719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bigrams for all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "28e0fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = nltk.collocations.BigramCollocationFinder.from_documents(factiva_cleaned)\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ebaaa7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store bigram measures in dict for easy access\n",
    "factiva_bigrams = dict(finder.score_ngrams(bigram_measures.raw_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "edcd2ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately the class Preprocessor does not remove all punct\n",
    "# so here we will filter it out of the dictionary\n",
    "filtered_factiva_bigrams = {k:v for (k,v) in factiva_bigrams.items() \n",
    "                            if k[0] != '--' and v > 0.0001}\n",
    "\n",
    "# sort dictionary by value\n",
    "filtered_factiva_bigrams = dict(sorted(filtered_factiva_bigrams.items(), \n",
    "                                       key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "64fb73d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty print sorted dictionary \n",
    "# and write to file\n",
    "pp = pprint.PrettyPrinter(indent=2, stream=open(\"outputs/factiva_bigrams.txt\",'w'), sort_dicts=False)\n",
    "pp.pprint(filtered_factiva_bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "600155eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top 100 bigrams by bmi\n",
    "finder.apply_freq_filter(3)\n",
    "finder.nbest(bigram_measures.pmi, 100)\n",
    "\n",
    "# write to file\n",
    "with open(\"outputs/factiva_bigram_pmi\", 'w', encoding='utf-8') as f:\n",
    "    for item in list(finder.nbest(bigram_measures.pmi, 100)):\n",
    "        f.write(f'{item}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ab22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create trigrams for all documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "22ce1303",
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_tri = nltk.collocations.TrigramCollocationFinder.from_documents(factiva_cleaned)\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9921b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store trigrams measures in dict for easy access\n",
    "factiva_trigrams = dict(finder_tri.score_ngrams(trigram_measures.raw_freq))\n",
    "\n",
    "# filter trigrams to remove punct\n",
    "filtered_factiva_trigrams = {k:v for (k,v) in factiva_trigrams.items() \n",
    "                            if k[0] != '--' and v > 0.0001}\n",
    "# sort dictionary by value\n",
    "filtered_factiva_trigrams = dict(sorted(filtered_factiva_trigrams.items(), \n",
    "                                       key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b8765b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty print sorted dictionary \n",
    "# and write to file\n",
    "pp = pprint.PrettyPrinter(indent=2, stream=open(\"outputs/factiva_trigrams.txt\",'w'), sort_dicts=False)\n",
    "pp.pprint(filtered_factiva_trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "fdb50411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top 100 trigrams by bmi\n",
    "finder_tri.apply_freq_filter(3)\n",
    "finder_tri.nbest(trigram_measures.pmi, 100)\n",
    "\n",
    "# write to file\n",
    "with open(\"outputs/factiva_trigram_pmi\", 'w', encoding='utf-8') as f:\n",
    "    for item in list(finder_tri.nbest(trigram_measures.pmi, 100)):\n",
    "        f.write(f'{item}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also follow up with quadgrams and so on \n",
    "# but ideally remove irrelevant entity names & proper nouns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frontline",
   "language": "python",
   "name": "frontline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

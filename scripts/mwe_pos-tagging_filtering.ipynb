{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289cdfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check working directory\n",
    "import os\n",
    "os.getcwd() # if directory is subfolder, change to home\n",
    "os.chdir('/home/sukayna/Documents/github/newspaper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a6df264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import usual packages\n",
    "import json\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "import datetime as dt\n",
    "import locale\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9626beae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sukayna/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download POS tagger from NLTK\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa7e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# always use json to load the corpus\n",
    "with open('data/factiva_data.json', 'r') as f:\n",
    "    factiva_corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1a1a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (with pos tagger component activated)\n",
    "spacy_mod = spacy.load(\"de_core_news_lg\",\n",
    "                 disable=['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbe2c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create smaller set of acceptable stopwords\n",
    "# remove verbs, pronouns and connectors with causal meaning from stopwords\n",
    "spacy_mod.Defaults.stop_words -= {'kam', 'sollte', 'dich', 'achte', 'daraus', 'dir', 'werdet', 'seid', 'unser', 'macht', 'deswegen',\n",
    "                                  'außerdem', 'damit', 'habe', 'können', 'könnt', 'hatte', 'werde', 'andere', 'deiner', 'meines',\n",
    "                                  'niemandem', 'achten', 'dürft', 'rechten', 'machte', 'dahinter', 'sah', 'seinen', 'dementsprechend',\n",
    "                                  'kann', 'muß', 'wäre', 'geworden', 'wegen', 'machen', 'waren', 'dürfen', 'dein', 'mögen', 'würde',\n",
    "                                  'musst', 'magst', 'ihren', 'aber', 'möchte', 'ihr', 'wir', 'allerdings', 'jedem', 'nicht', 'ihres',\n",
    "                                  'kommt', 'gibt', 'infolgedessen', 'mögt', 'doch', 'sollten', 'seine', 'keine', 'wollte', 'ich',\n",
    "                                  'müssen', 'wollten', 'warum', 'ist', 'ihnen', 'mein', 'mochte', 'geht', 'trotzdem', 'gab', 'durfte',\n",
    "                                  'dagegen', 'sie', 'sind', 'wart', 'wer', 'haben', 'du', 'werden', 'eigene', 'ihn', 'seien', 'eigen',\n",
    "                                  'meinen', 'seiner', 'hatten', 'müsst', 'wollen', 'indem', 'wollt', 'gehabt', 'deine',  'denn',\n",
    "                                  'nachdem', 'konnte', 'ihrer', 'seinem', 'gemusst', 'bin', 'währenddem', 'dank', 'willst', 'würden',\n",
    "                                  'gemocht',  'hätten', 'demzufolge', 'seines', 'mussten', 'nahm', 'daher', 'darauf', 'ging', 'mochten',\n",
    "                                  'meinem', 'darum', 'gedurft', 'wurden', 'bist', 'ihrem', 'gehen', 'sein', 'kannst', 'gewollt',\n",
    "                                  'könnte',  'heisst', 'neben', 'meiner', 'euch', 'darfst', 'deshalb', 'konnten', 'ausserdem', 'ihm',\n",
    "                                  'tun', 'gekannt',  'worden', 'habt', 'darf', 'demgegenüber', 'gewesen', 'sollen', 'soll', 'kommen',\n",
    "                                  'tat', 'jahre'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c28a6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 2564/2564 [00:59<00:00, 42.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert corpus to language object\n",
    "factiva_spacy = []\n",
    "for doc in tqdm(factiva_corpus):\n",
    "    factiva_spacy.append(spacy_mod(doc['body']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6a8cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for preprocessing (does not remove stopwords)\n",
    "def preprocess(doc: str, remove_ent=False):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        doc (str): String text\n",
    "        remove_ent (bool, optional): If True, removes entities using spaCy. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        doc_preprocessed (list): Preprocessed lower-case corpus with punctuation, non-alphanumeric characters, spaCy stopwords and proper nouns removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    if remove_ent == True:\n",
    "        doc_no_ent = []\n",
    "        ents = [e.text for e in doc.ents]\n",
    "        for item in doc:\n",
    "            if item.text in ents:\n",
    "                pass\n",
    "            else:\n",
    "                doc_no_ent.append(item)\n",
    "\n",
    "        doc_preprocessed = [token.lower_ for token in doc_no_ent if\n",
    "                            # token is not punctuation\n",
    "                            token.is_punct == False and\n",
    "                            # token is alphanumeric character\n",
    "                            token.is_alpha == True and\n",
    "                            # token is not proper noun\n",
    "                            token.pos_ != \"PROPN\"]\n",
    "        \n",
    "    else: # do not remove entities\n",
    "        doc_preprocessed = [token.lower_ for token in doc if\n",
    "                            # token is not punctuation\n",
    "                            token.is_punct == False and\n",
    "                            # token is alphanumeric character\n",
    "                            token.is_alpha == True and\n",
    "                            # token is not proper noun\n",
    "                            token.pos_ != \"PROPN\"]\n",
    "\n",
    "    return doc_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "350a53ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 2564/2564 [00:01<00:00, 2486.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocess (only minimal cleaning!)\n",
    "factiva_cleaned = []\n",
    "for doc in tqdm(factiva_spacy): \n",
    "    factiva_cleaned.append(preprocess(doc, remove_ent=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76130e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1032054"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unlist corpus for extracting ngrams from words instead of docs\n",
    "unlisted = [item for items in factiva_cleaned for item in items]\n",
    "len(unlisted) # total tokens in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2266fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise bigram finder and assoc measures\n",
    "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(unlisted)\n",
    "bigrams = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a11c2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract frequency\n",
    "bigram_freq = bigramFinder.ngram_fd.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d43b3791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bigram freq table\n",
    "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f48dea7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(in, der)</td>\n",
       "      <td>4278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(häuslicher, gewalt)</td>\n",
       "      <td>3375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(in, den)</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(häusliche, gewalt)</td>\n",
       "      <td>1886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(für, die)</td>\n",
       "      <td>1531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 bigram  freq\n",
       "0             (in, der)  4278\n",
       "1  (häuslicher, gewalt)  3375\n",
       "2             (in, den)  2002\n",
       "3   (häusliche, gewalt)  1886\n",
       "4            (für, die)  1531"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigramFreqTable.head().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f6ef14",
   "metadata": {},
   "source": [
    "### Use POS tags to filter bigrams \n",
    "\n",
    "Note: needs to be improved/fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdee4c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get english stopwords\n",
    "de_stopwords = set(spacy_mod.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1518dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter for ADJ/NN bigrams\n",
    "def rightTypes(ngram):\n",
    "    if '-pron-' in ngram or '' in ngram or ' 'in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in de_stopwords:\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da4bf5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter bigrams\n",
    "filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ae69f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>(häuslicher, gewalt)</td>\n",
       "      <td>3375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3046</th>\n",
       "      <td>(häusliche, gewalt)</td>\n",
       "      <td>1886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>(wegen, häuslicher)</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>(opfer, häuslicher)</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4810</th>\n",
       "      <td>(häuslichen, gewalt)</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3617</th>\n",
       "      <td>(sie, nicht)</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7242</th>\n",
       "      <td>(seine, frau)</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11031</th>\n",
       "      <td>(aber, nicht)</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20420</th>\n",
       "      <td>(wir, haben)</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4425</th>\n",
       "      <td>(ich, habe)</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bigram  freq\n",
       "1779   (häuslicher, gewalt)  3375\n",
       "3046    (häusliche, gewalt)  1886\n",
       "1778    (wegen, häuslicher)   524\n",
       "2137    (opfer, häuslicher)   395\n",
       "4810   (häuslichen, gewalt)   292\n",
       "3617           (sie, nicht)   281\n",
       "7242          (seine, frau)   274\n",
       "11031         (aber, nicht)   274\n",
       "20420          (wir, haben)   274\n",
       "4425            (ich, habe)   259"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_bi[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frontline",
   "language": "python",
   "name": "frontline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

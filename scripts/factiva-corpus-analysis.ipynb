{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start\n",
    "\n",
    "- Set working directory\n",
    "- Load modules\n",
    "- Load data\n",
    "- Set datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check working directory\n",
    "import os\n",
    "os.getcwd() # if directory is subfolder, change to home\n",
    "os.chdir('/Users/nhuquynh/Documents/Frontline/newspaper/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhuquynh/Library/Python/3.8/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import json \n",
    "import datetime as dt\n",
    "import locale\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(\"data/factiva_data.json\", 'r') as f:\n",
    "    factiva_corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de_DE.UTF-8'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change date format\n",
    "locale.setlocale(locale.LC_ALL, 'de_DE.UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing \n",
    "\n",
    "- Convert time data to datetime format \n",
    "- Load spaCy model for tokenization and preprocessing\n",
    "- Remove punctuation, symbols and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string dates to datetime format\n",
    "# Year-Month-Day HH:MM:SS is the default output for string dates\n",
    "\n",
    "for doc in factiva_corpus: \n",
    "    doc['date'] = dt.datetime.strptime(doc['date'], \"%d %B %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 2410/2564 [01:28<00:05, 27.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nhuquynh/Documents/Frontline/newspaper/scripts/factiva-corpus-analysis.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nhuquynh/Documents/Frontline/newspaper/scripts/factiva-corpus-analysis.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m factiva_spacy \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nhuquynh/Documents/Frontline/newspaper/scripts/factiva-corpus-analysis.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m tqdm(factiva_corpus):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nhuquynh/Documents/Frontline/newspaper/scripts/factiva-corpus-analysis.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     factiva_spacy\u001b[39m.\u001b[39mappend(spacy_mod(doc[\u001b[39m'\u001b[39;49m\u001b[39mbody\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/spacy/language.py:1020\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1019\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1020\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1022\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/spacy/pipeline/tok2vec.py:125\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     width \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mget_dim(\u001b[39m\"\u001b[39m\u001b[39mnO\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39malloc((\u001b[39m0\u001b[39m, width)) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs]\n\u001b[0;32m--> 125\u001b[0m tokvecs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(docs)\n\u001b[1;32m    126\u001b[0m batch_id \u001b[39m=\u001b[39m Tok2VecListener\u001b[39m.\u001b[39mget_batch_id(docs)\n\u001b[1;32m    127\u001b[0m \u001b[39mfor\u001b[39;00m listener \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlisteners:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/thinc/layers/with_array.py:38\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m](Xseq, is_train)\n\u001b[1;32m     37\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _list_forward(model, Xseq, is_train))\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/thinc/layers/with_array.py:73\u001b[0m, in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     71\u001b[0m lengths \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39masarray1i([\u001b[39mlen\u001b[39m(seq) \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m Xs])\n\u001b[1;32m     72\u001b[0m Xf \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mflatten(Xs, pad\u001b[39m=\u001b[39mpad)\n\u001b[0;32m---> 73\u001b[0m Yf, get_dXf \u001b[39m=\u001b[39m layer(Xf, is_train)\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dYs: ListXd) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ListXd:\n\u001b[1;32m     76\u001b[0m     dYf \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mflatten(dYs, pad\u001b[39m=\u001b[39mpad)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/thinc/layers/residual.py:41\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[39mreturn\u001b[39;00m d_output \u001b[39m+\u001b[39m dX\n\u001b[0;32m---> 41\u001b[0m Y, backprop_layer \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mlayers[\u001b[39m0\u001b[39;49m](X, is_train)\n\u001b[1;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(X, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m [X[i] \u001b[39m+\u001b[39m Y[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X))], backprop\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "    \u001b[0;31m[... skipping similar frames: Model.__call__ at line 291 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/thinc/layers/maxout.py:49\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     47\u001b[0m W \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_param(\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m W \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape2f(W, nO \u001b[39m*\u001b[39m nP, nI)\n\u001b[0;32m---> 49\u001b[0m Y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mgemm(X, W, trans2\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     50\u001b[0m Y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape1f(b, nO \u001b[39m*\u001b[39m nP)\n\u001b[1;32m     51\u001b[0m Z \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape3f(Y, Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], nO, nP)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "spacy_mod = spacy.load(\"de_core_news_lg\",\n",
    "                 disable=['ner', 'parser', 'tagger'])\n",
    "\n",
    "# Turn to Language object\n",
    "factiva_spacy = []\n",
    "for doc in tqdm(factiva_corpus):\n",
    "    factiva_spacy.append(spacy_mod(doc['body']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'richtig', 'erster', 'neuntes', 'jenen', 'erst', 'hätte', 'einen', 'kam', 'mancher', 'sollte', 'dafür', 'dermaßen', 'ganzer', 'dich', 'achte', 'daraus', 'dir', 'zurück', 'werdet', 'nur', 'derjenige', 'denen', 'darin', 'wem', 'dieselbe', 'gegenüber', 'weiteren', 'seid', 'auch', 'unser', 'drin', 'macht', 'demselben', 'diesem', 'zweiten', 'außerdem', 'deswegen', 'daß', 'geschweige', 'zum', 'damit', 'jetzt', 'habe', 'können', 'en', 'jemand', 'könnt', 'hatte', 'jeder', 'werde', 'jede', 'eigenes', 'andere', 'einigen', 'lang', 'deiner', 'meines', 'neuen', 'in', 'siebenter', 'niemandem', 'keinem', 'einiges', 'achten', 'diejenigen', 'solcher', 'dürft', 'diese', 'dritten', 'rechten', 'machte', 'andern', 'drei', 'dahinter', 'sah', 'jahre', 'seinen', 'sondern', 'dementsprechend', 'grosse', 'manchen', 'wen', 'kann', 'im', 'siebentes', 'desselben', 'muß', 'vom', 'zur', 'wäre', 'geworden', 'drittes', 'dessen', 'auf', 'unsere', 'wegen', 'welcher', 'á', 'ersten', 'kleiner', 'fünften', 'allgemeinen', 'etwas', 'entweder', 'offen', 'es', 'achter', 'machen', 'über', 'mehr', 'ja', 'ag', 'die', 'kleines', 'sechster', 'sechsten', 'alles', 'nein', 'ende', 'siebte', 'um', 'fünfter', 'genug', 'wirklich', 'hat', 'zugleich', 'jedoch', 'durchaus', 'zehnter', 'muss', 'musste', 'nie', 'wahr', 'wenige', 'immer', 'vor', 'gute', 'jemandem', 'was', 'weiter', 'überhaupt', 'sehr', 'davon', 'hinter', 'einmaleins', 'waren', 'dürfen', 'dein', 'dritte', 'dazu', 'dasselbe', 'mögen', 'würde', 'erstes', 'endlich', 'musst', 'solchen', 'ihren', 'magst', 'zusammen', 'aber', 'dasein', 'allein', 'möchte', 'allen', 'eigener', 'ihr', 'bis', 'wir', 'großer', 'allerdings', 'jedem', 'nicht', 'ihres', 'dadurch', 'demgemäss', 'a', 'kommt', 'wenigstens', 'gibt', 'infolgedessen', 'sechstes', 'mögt', 'fünfte', 'zehnten', 'denselben', 'seit', 'schon', 'vergangenen', 'doch', 'siebtes', 'zwar', 'demgemäß', 'sollten', 'seine', 'teil', 'jahr', 'irgend', 'keine', 'statt', 'selbst', 'weniger', 'keinen', 'wollte', 'zeit', 'zweite', 'her', 'gutes', 'rechtes', 'gekonnt', 'eigenen', 'dass', 'ihre', 'durften', 'tel', 'neunten', 'möglich', 'derselben', 'war', 'zweites', 'den', 'kaum', 'sonst', 'uns', 'sei', 'ein', 'solche', 'dieselben', 'deinem', 'siebenten', 'wieder', 'ich', 'müssen', 'neunte', 'siebente', 'wollten', 'einmal', 'davor', 'warum', 'welches', 'sowie', 'ist', 'ab', 'ihnen', 'hin', 'mein', 'vierte', 'dieser', 'mochte', 'für', 'geht', 'trotzdem', 'einiger', 'groß', 'natürlich', 'gab', 'durfte', 'anders', 'und', 'heute', 'dagegen', 'rund', 'bei', 'als', 'da', 'vielen', 'zwei', 'außer', 'daran', 'ob', 'fünftes', 'sie', 'ehrlich', 'schlecht', 'na', 'eines', 'zunächst', 'bekannt', 'sich', 'sind', 'wart', 'uhr', 'deren', 'ins', 'dort', 'jene', 'kurz', 'acht', 'also', 'dies', 'derselbe', 'das', 'aus', 'einander', 'kleinen', 'mittel', 'rechter', 'welchen', 'nun', 'noch', 'wer', 'wirst', 'allem', 'will', 'etwa', 'unserer', 'elf', 'alle', 'gross', 'recht', 'erste', 'siebter', 'gar', 'gegen', 'vierter', 'gesagt', 'fünf', 'haben', 'beiden', 'einem', 'du', 'lange', 'derjenigen', 'dahin', 'unter', 'werden', 'wann', 'neue', 'wessen', 'durch', 'besser', 'ganzen', 'anderem', 'diesen', 'welche', 'darüber', 'später', 'eigene', 'ihn', 'jemanden', 'guter', 'vielleicht', 'seien', 'eben', 'hast', 'während', 'niemanden', 'eine', 'solang', 'eigen', 'sagt', 'grosser', 'von', 'beispiel', 'besonders', 'meinen', 'seiner', 'an', 'hatten', 'müsst', 'wollen', 'indem', 'wollt', 'ausser', 'sechs', 'währenddessen', 'gehabt', 'deine', 'zehn', 'darunter', 'denn', 'nachdem', 'bald', 'konnte', 'sieben', 'neun', 'ihrer', 'seinem', 'gemusst', 'bin', 'tagen', 'viel', 'wo', 'siebten', 'währenddem', 'dank', 'oder', 'vielem', 'sechste', 'würden', 'welchem', 'beide', 'ohne', 'willst', 'wird', 'einige', 'wenn', 'diejenige', 'ganz', 'oben', 'gern', 'wie', 'großes', 'manches', 'beim', 'gemacht', 'mir', 'wurde', 'wenig', 'zwischen', 'heißt', 'so', 'danach', 'satt', 'dann', 'oft', 'sagte', 'seitdem', 'weiteres', 'meine', 'weil', 'er', 'am', 'gemocht', 'einer', 'hätten', 'demzufolge', 'damals', 'morgen', 'seines', 'zehntes', 'daselbst', 'mussten', 'dieses', 'los', 'nahm', 'daher', 'darauf', 'großen', 'je', 'des', 'mag', 'weniges', 'dritter', 'viele', 'tag', 'weitere', 'mich', 'ging', 'mochten', 'meinem', 'darum', 'hoch', 'solches', 'jedermann', 'gedurft', 'wurden', 'jedermanns', 'ganzes', 'jener', 'bist', 'ihrem', 'jenes', 'niemand', 'zu', 'gehen', 'dazwischen', 'man', 'nichts', 'jahren', 'sein', 'früher', 'zwanzig', 'jeden', 'jenem', 'ach', 'dermassen', 'kannst', 'gewollt', 'manche', 'ebenso', 'bisher', 'könnte', 'gerade', 'vergangene', 'manchem', 'neunter', 'vier', 'mit', 'viertes', 'heisst', 'vierten', 'daneben', 'nach', 'anderen', 'neben', 'meiner', 'euch', 'kleine', 'darfst', 'deshalb', 'zweiter', 'übrigens', 'zuerst', 'konnten', 'ausserdem', 'große', 'ihm', 'solchem', 'gut', 'tun', 'gekannt', 'leicht', 'hier', 'worden', 'leider', 'besten', 'dabei', 'habt', 'weit', 'rechte', 'bereits', 'keiner', 'tage', 'darf', 'demgegenüber', 'gewesen', 'sollen', 'zehnte', 'achtes', 'gleich', 'dem', 'ganze', 'grossen', 'lieber', 'wohl', 'soll', 'kommen', 'der', 'kein', 'tat', 'grosses', 'aller'}\n"
     ]
    }
   ],
   "source": [
    "# Inspect spacy stopword list\n",
    "print(spacy_mod.Defaults.stop_words)\n",
    "\n",
    "# Save in object \n",
    "sw_list = spacy_mod.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for preprocessing\n",
    "def preprocess(doc: str, remove_ent=False):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        doc (str): String text\n",
    "        remove_ent (bool, optional): If True, removes entities using spaCy. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        doc_preprocessed (list): Preprocessed lower-case corpus with punctuation, non-alphanumeric characters, spaCy stopwords and proper nouns removed.\n",
    "    \"\"\"\n",
    "\n",
    "    if remove_ent == True:\n",
    "        doc_no_ent = []\n",
    "        ents = [e.text for e in doc.ents]\n",
    "        for item in doc:\n",
    "            if item.text in ents:\n",
    "                pass\n",
    "            else:\n",
    "                doc_no_ent.append(item)\n",
    "\n",
    "        doc_preprocessed = [token.lower_ for token in doc_no_ent if\n",
    "                            # token is not punctuation\n",
    "                            token.is_punct == False and\n",
    "                            # token is alphanumeric character\n",
    "                            token.is_alpha == True and\n",
    "                            # token is not stop word\n",
    "                            token.is_stop == False and\n",
    "                            # token is not proper noun\n",
    "                            token.pos_ != \"PROPN\"]\n",
    "\n",
    "    else:\n",
    "        doc_preprocessed = [token.lower_ for token in doc if\n",
    "                            # token is not punctuation\n",
    "                            token.is_punct == False and\n",
    "                            # token is alphanumeric character\n",
    "                            token.is_alpha == True and\n",
    "                            # token is not stop word\n",
    "                            token.is_stop == False and\n",
    "                            # token is not proper noun\n",
    "                            token.pos_ != \"PROPN\"]\n",
    "\n",
    "    return doc_preprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2410/2410 [00:01<00:00, 1354.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess corpus\n",
    "factiva_cleaned = []\n",
    "for doc in tqdm(factiva_spacy): \n",
    "    factiva_cleaned.append(preprocess(doc, remove_ent=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stark', 'blutenden', 'schnittverletzungen', 'alter', 'mann']\n",
      "2410\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# See example\n",
    "print(factiva_cleaned[5][0:5],\n",
    "      len(factiva_cleaned),\n",
    "      type(factiva_cleaned),\n",
    "      sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocation analysis \n",
    "\n",
    "- Collocation across all documents\n",
    "- Collocation across years\n",
    "- Calculate scores (rawfreq + PMI + chisq?) for comparison\n",
    "- Plot comparisons over time: association strength (dot chart) / netowrk  graphs / biplots (using semantic similarity)\n",
    "- Check for collocation strength\n",
    "- Significance testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collocation across all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bigram collocations across all documents\n",
    "finder_bi = nltk.collocations.BigramCollocationFinder.from_documents(\n",
    "    factiva_cleaned)\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# Store bigram measures in dict for easy access\n",
    "factiva_bigrams = dict(finder_bi.score_ngrams(bigram_measures.raw_freq))\n",
    "\n",
    "# Sort bigram dictionary by value\n",
    "filtered_factiva_bigrams = dict(sorted(factiva_bigrams.items(),\n",
    "                                       key=lambda item: item[1],\n",
    "                                       reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trigram collocations across all documents\n",
    "finder_tri = nltk.collocations.TrigramCollocationFinder.from_documents(\n",
    "    factiva_cleaned)\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# Store trigram measures in dict\n",
    "factiva_trigrams = dict(finder_tri.score_ngrams(trigram_measures.raw_freq))\n",
    "\n",
    "# Sort trigram dict by value\n",
    "filtered_factiva_trigrams = dict(sorted(factiva_trigrams.items(),\n",
    "                                        key=lambda item: item[1], \n",
    "                                        reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store information in file\n",
    "\n",
    "# Bigrams\n",
    "pp = pprint.PrettyPrinter(indent=2, stream=open(\n",
    "    \"outputs/spacy_stopwords/factiva_bigrams.txt\", 'w'), sort_dicts=False)\n",
    "pp.pprint(filtered_factiva_bigrams)\n",
    "\n",
    "# Trigrams\n",
    "pp = pprint.PrettyPrinter(indent=2, stream=open(\n",
    "    \"outputs/spacy_stopwords/factiva_trigrams.txt\", 'w'), sort_dicts=False)\n",
    "pp.pprint(filtered_factiva_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('praxiserfahrung', 'tücken', 'gerichtsfesten'),\n",
       " ('lokaljournalismus', 'datengetriebene', 'investigative'),\n",
       " ('gemeinnützigen', 'recherchezentrums', 'correctiv'),\n",
       " ('bundestrainerin', 'bundestrainer', 'kapitän'),\n",
       " ('zwängen', 'depressiven', 'verstimmungen'),\n",
       " ('lokalredaktionen', 'umsetzt', 'gemeinnützigen'),\n",
       " ('umsetzt', 'gemeinnützigen', 'recherchezentrums'),\n",
       " ('erwarb', 'halbautomatisches', 'gewehr'),\n",
       " ('klinikärzte', 'gynäkologen', 'hausärzte'),\n",
       " ('etappensieg', 'kiosk', 'bahnhof'),\n",
       " ('hauptanteil', 'leichtverletzte', 'ausgemacht'),\n",
       " ('studium', 'praxiserfahrung', 'tücken'),\n",
       " ('rechtspopulistischer', 'abgeordneter', 'slowakische'),\n",
       " ('reuben', 'garantien', 'doug'),\n",
       " ('belebten', 'bahnsteig', 'jungfernstieg'),\n",
       " ('halbautomatisches', 'gewehr', 'pistolen'),\n",
       " ('bremischen', 'zentralstelle', 'verwirklichung'),\n",
       " ('filmreihe', 'phantastische', 'tierwesen'),\n",
       " ('überstellung', 'ermittlungsrichter', 'vorgeführt'),\n",
       " ('first', 'come', 'first'),\n",
       " ('rücksicht', 'untersuche', 'vermerken'),\n",
       " ('dunkelblonden', 'kinnlangen', 'haaren'),\n",
       " ('lesbisch', 'schwul', 'bisexuell'),\n",
       " ('erhellen', 'bunte', 'solarlichter'),\n",
       " ('erfahrenen', 'psychotherapeuten', 'fachärzten'),\n",
       " ('depressiven', 'verstimmungen', 'seelischen'),\n",
       " ('bereitschaftsdienst', 'kassenärztlichen', 'vereinigung'),\n",
       " ('tücken', 'gerichtsfesten', 'medizinischen'),\n",
       " ('messerangriff', 'belebten', 'bahnsteig'),\n",
       " ('öfteren', 'zeitverzug', 'anzeigt'),\n",
       " ('örtliche', 'hilfsgruppen', 'paralysiert'),\n",
       " ('datengetriebene', 'investigative', 'recherchen'),\n",
       " ('rechtsmediziner', 'spermaspuren', 'hautschuppen'),\n",
       " ('spielervereinigung', 'atp', 'tennisprofis'),\n",
       " ('absolutes', 'no', 'go'),\n",
       " ('buzz', 'feed', 'news'),\n",
       " ('landratsamtes', 'verschiedensten', 'lebensbereiche'),\n",
       " ('stellwerk', 'bahnhofsvorplatz', 'wusterhausen'),\n",
       " ('psychotherapeuten', 'fachärzten', 'psychosomatische'),\n",
       " ('kaufen', 'erwarb', 'halbautomatisches'),\n",
       " ('sozialminister', 'manne', 'lucha'),\n",
       " ('fachärzten', 'psychosomatische', 'medizin'),\n",
       " ('hochschule', 'angewandte', 'wissenschaften'),\n",
       " ('georgische', 'tennisprofi', 'mutmaßlicher'),\n",
       " ('let', 'talk', 'wettbewerb'),\n",
       " ('the', 'british', 'empire'),\n",
       " ('telefonberatung', 'familienzentrum', 'gesprächsbedarf'),\n",
       " ('räumen', 'frauentreffs', 'jupiterstraße'),\n",
       " ('unbekleideten', 'körperoberflächen', 'hände'),\n",
       " ('historisch', 'ungleichen', 'machtverhältnisse'),\n",
       " ('ängsten', 'zwängen', 'depressiven'),\n",
       " ('anriefen', 'verdoppelt', 'gesundheitsdienste'),\n",
       " ('hilfsdienste', 'anriefen', 'verdoppelt'),\n",
       " ('serbe', 'rande', 'finals'),\n",
       " ('sonstige', 'sprechzeit', 'montags'),\n",
       " ('talk', 'wettbewerb', 'verständliche'),\n",
       " ('oberstleutnant', 'stamminger', 'landespolizeidirektion'),\n",
       " ('fussfessel', 'elektronisches', 'armband'),\n",
       " ('fox', 'news', 'flugzeug'),\n",
       " ('schlafentzug', 'typische', 'methode'),\n",
       " ('befund', 'halbwegs', 'fähigen'),\n",
       " ('leichte', 'abnahme', 'gewaltschutzsachen'),\n",
       " ('madrider', 'oberlandesgerichts', 'verteidigt'),\n",
       " ('filderstadt', 'ostfildern', 'neuhausen'),\n",
       " ('symptome', 'bereitschaftsdienst', 'kassenärztlichen'),\n",
       " ('verstimmungen', 'seelischen', 'nöten'),\n",
       " ('würgemale', 'verheilt', 'staatsanwälte'),\n",
       " ('verteidiger', 'nu', 'zerpflückt'),\n",
       " ('gewehr', 'pistolen', 'gewehr'),\n",
       " ('zielfahndung', 'bundeskriminalamtes', 'lokalisiert'),\n",
       " ('rheinisch', 'bergischen', 'kreises'),\n",
       " ('übernimmt', 'polizeiaufgaben', 'untersteht'),\n",
       " ('eingestochen', 'zill', 'tatwaffe'),\n",
       " ('big', 'little', 'lies'),\n",
       " ('seniorentelefon', 'engagement', 'kompass'),\n",
       " ('untersucht', 'unbekleideten', 'körperoberflächen'),\n",
       " ('verberge', 'erhebliches', 'dunkelfeld'),\n",
       " ('and', 'half', 'men'),\n",
       " ('two', 'and', 'half'),\n",
       " ('pressemitteilungen', 'pts', 'jeweiligen'),\n",
       " ('pts', 'jeweiligen', 'aussender'),\n",
       " ('kantonalen', 'sozialdirektorinnen', 'sozialdirektoren'),\n",
       " ('überschwemmt', 'unzureichend', 'ausgestattet'),\n",
       " ('bundestrainer', 'kapitän', 'neuer'),\n",
       " ('save', 'the', 'children'),\n",
       " ('netzwerk', 'lokaljournalismus', 'datengetriebene'),\n",
       " ('rechtsnormen', 'angewandt', 'vorgesehenen'),\n",
       " ('verhältnis', 'schwerverletzten', 'gleichbleibend'),\n",
       " ('beratungstelefon', 'psychotherapeutische', 'telefonberatung'),\n",
       " ('haaren', 'spürt', 'wachem'),\n",
       " ('kinnlangen', 'haaren', 'spürt'),\n",
       " ('staatsanwälte', 'anfangen', 'krankenakte'),\n",
       " ('interaktive', 'karte', 'bestätigten'),\n",
       " ('diskutieren', 'let', 'talk'),\n",
       " ('anwältin', 'camille', 'vasquez'),\n",
       " ('fähigen', 'verteidiger', 'nu'),\n",
       " ('recherchezentrums', 'correctiv', 'spenden'),\n",
       " ('starteten', 'teams', 'aufruf'),\n",
       " ('nähen', 'verbinden', 'heilen'),\n",
       " ('polizeiaufgaben', 'untersteht', 'innenministerium')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract top 100 bigrams by bmi\n",
    "finder_bi.apply_freq_filter(3)\n",
    "finder_bi.nbest(bigram_measures.pmi, 100)\n",
    "\n",
    "# extract top 100 trigrams by bmi\n",
    "finder_tri.apply_freq_filter(3)\n",
    "finder_tri.nbest(trigram_measures.pmi, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store 100 top bigrams in file\n",
    "with open(\"outputs/spacy_stopwords/factiva_bigram_pmi\", 'w', encoding='utf-8') as f:\n",
    "    for item in list(finder_bi.nbest(bigram_measures.pmi, 100)):\n",
    "        f.write(f'{item}\\n')\n",
    "\n",
    "# Store 100 top trigrams in file\n",
    "with open(\"outputs/spacy_stopwords/factiva_trigram_pmi\", 'w', encoding='utf-8') as f:\n",
    "    for item in list(finder_tri.nbest(trigram_measures.pmi, 100)):\n",
    "        f.write(f'{item}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter trigrams / bigrams with certain number of stopwords\n",
    "\n",
    "factiva_trigrams_test = []\n",
    "\n",
    "for doc in list(finder_tri.nbest(trigram_measures.pmi, 100)):\n",
    "    count = 0\n",
    "    for w in doc: \n",
    "        if w in sw_list:\n",
    "            count += 1 \n",
    "    if count <= 1: \n",
    "        factiva_trigrams_test.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factiva_trigrams_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collocation across years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get documents per year\n",
    "\n",
    "factiva_2017 = [x for x in factiva_corpus if x['date'].year == 2017]\n",
    "factiva_2018 = [x for x in factiva_corpus if x['date'].year == 2018]\n",
    "factiva_2019 = [x for x in factiva_corpus if x['date'].year == 2019]\n",
    "factiva_2020 = [x for x in factiva_corpus if x['date'].year == 2020]\n",
    "factiva_2021 = [x for x in factiva_corpus if x['date'].year == 2021]\n",
    "factiva_2022 = [x for x in factiva_corpus if x['date'].year == 2022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot frequencies \n",
    "\n",
    "# Turn to pandas df for plot and focus on years \n",
    "df_years = pd.DataFrame.from_dict(factiva_corpus, orient='columns')\n",
    "df_years['date'] = pd.DatetimeIndex(df_years['date']).year\n",
    "plot_years = df_years.groupby(['date'])['title'].count()\n",
    "\n",
    "# Plot as bar chart \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot legend\n",
    "bars = ax.bar(plot_years.index, plot_years)\n",
    "ax.bar_label(bars)\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Sum of DA articles')\n",
    "\n",
    "# Plot design\n",
    "ax.set_xticks(plot_years.index)\n",
    "ax.set_yticks(range(200, 800, 200))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess documents for every year 2017 - 2022\n",
    "\n",
    "# Turn to Language object\n",
    "factiva_spacy_2017 = []\n",
    "for doc in tqdm(factiva_2017):\n",
    "    factiva_spacy_2017.append(spacy_mod(doc['body']))\n",
    "factiva_spacy_2018 = []\n",
    "for doc in tqdm(factiva_2018):\n",
    "    factiva_spacy_2018.append(spacy_mod(doc['body']))\n",
    "factiva_spacy_2019 = []\n",
    "for doc in tqdm(factiva_2019):\n",
    "    factiva_spacy_2019.append(spacy_mod(doc['body']))\n",
    "factiva_spacy_2020 = []\n",
    "for doc in tqdm(factiva_2020):\n",
    "    factiva_spacy_2020.append(spacy_mod(doc['body']))\n",
    "factiva_spacy_2021 = []\n",
    "for doc in tqdm(factiva_2021):\n",
    "    factiva_spacy_2021.append(spacy_mod(doc['body']))\n",
    "factiva_spacy_2022 = []\n",
    "for doc in tqdm(factiva_2022):\n",
    "    factiva_spacy_2022.append(spacy_mod(doc['body']))\n",
    "\n",
    "# Preprocess corpus\n",
    "factiva_cleaned_2017 = []\n",
    "for doc in tqdm(factiva_spacy_2017): \n",
    "    factiva_cleaned_2017.append(preprocess(doc, remove_ent=True))\n",
    "factiva_cleaned_2018 = []\n",
    "for doc in tqdm(factiva_spacy_2018): \n",
    "    factiva_cleaned_2018.append(preprocess(doc, remove_ent=True))\n",
    "factiva_cleaned_2019 = []\n",
    "for doc in tqdm(factiva_spacy_2019): \n",
    "    factiva_cleaned_2019.append(preprocess(doc, remove_ent=True))\n",
    "factiva_cleaned_2020 = []\n",
    "for doc in tqdm(factiva_spacy_2020): \n",
    "    factiva_cleaned_2020.append(preprocess(doc, remove_ent=True))\n",
    "factiva_cleaned_2021 = []\n",
    "for doc in tqdm(factiva_spacy_2021): \n",
    "    factiva_cleaned_2021.append(preprocess(doc, remove_ent=True))\n",
    "factiva_cleaned_2022 = []\n",
    "for doc in tqdm(factiva_spacy_2022): \n",
    "    factiva_cleaned_2022.append(preprocess(doc, remove_ent=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocation_docs(doc:list, filename:str=\"\", filename_top100:str=\"\"):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        doc (list): Nested list of strings \n",
    "        filename (str, optional): Filename for list of bigrams. Defaults to \"\" for no export.\n",
    "        filename_top100 (str, optional): Filename for list of top 100 bigrams by PMI. Defaults to \"\" for no export.\n",
    "\n",
    "    Returns:\n",
    "        doc_filtered_bigrams (dict): Dictionary of bigrams and scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create collocations\n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_documents(doc)\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    doc_bigrams = dict(finder.score_ngrams(bigram_measures.raw_freq))\n",
    "\n",
    "    # Return as dict\n",
    "    doc_filtered_bigrams = dict(sorted(doc_bigrams.items(),\n",
    "                                       key=lambda item: item[1],\n",
    "                                       reverse=True))\n",
    "\n",
    "    # If file name given, write information to file in outputs folder\n",
    "    if not filename:\n",
    "        pass\n",
    "    else:\n",
    "        pp = pprint.PrettyPrinter(indent=2, stream=open(\n",
    "            str(\"outputs/\"+filename+\".txt\"), 'w'),\n",
    "            sort_dicts=False)\n",
    "        pp.pprint(doc_filtered_bigrams)\n",
    "    \n",
    "    # If file name given, write top 100 bigrams by PMI to file in outputs folder\n",
    "    if not filename_top100:\n",
    "        pass\n",
    "    else:\n",
    "        finder.apply_freq_filter(3)\n",
    "        with open(str(\"outputs/\"+filename_top100), 'w', encoding='utf-8') as f:\n",
    "            for item in list(finder.nbest(bigram_measures.pmi, 100)):\n",
    "                f.write(f'{item}\\n')\n",
    "\n",
    "        return doc_filtered_bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram collocation analysis for every year\n",
    "\n",
    "collocation_docs(factiva_cleaned_2017, \"factiva_2017\", \"factive_2017_top100\")\n",
    "collocation_docs(factiva_cleaned_2018, \"factiva_2018\", \"factive_2018_top100\")\n",
    "collocation_docs(factiva_cleaned_2019, \"factiva_2019\", \"factive_2019_top100\")\n",
    "collocation_docs(factiva_cleaned_2020, \"factiva_2020\", \"factive_2020_top100\")\n",
    "collocation_docs(factiva_cleaned_2021, \"factiva_2021\", \"factive_2021_top100\")\n",
    "collocation_docs(factiva_cleaned_2022, \"factiva_2022\", \"factive_2022_top100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate scores for comparison\n",
    "\n",
    "- raw frequencies \n",
    "- PMI \n",
    "- chisquare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END OF CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

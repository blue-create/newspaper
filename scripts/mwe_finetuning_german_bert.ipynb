{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c9b6d7",
   "metadata": {},
   "source": [
    "# Finetuning the BERT-base-german-cased model\n",
    "\n",
    "About the model: https://huggingface.co/bert-base-german-cased\n",
    "\n",
    "In this example, we will be fine-tuning a transformer model for the multiclass text classification problem, e.g., classifying sentences/news headlines into provided categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b611dc",
   "metadata": {},
   "source": [
    "Hardware requirements:\n",
    "\n",
    "    Python 3.6 and above\n",
    "    Pytorch, Transformers and general use Python ML Libraries\n",
    "    GPU enabled setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a426260",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install modules\n",
    "\n",
    "!pip install torch \n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3717da1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhuquynh/Documents/Frontline/newspaper/env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries \n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9c1aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up device for GPU usage \n",
    "# (check options if using Google Colab or Cloud!)\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef24b1",
   "metadata": {},
   "source": [
    "### Load preprocessed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba87faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for data preprocessing for modelling\n",
    "# load in preprocessed dataset here!\n",
    "# remember to encode the categories appropriately!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad76a46",
   "metadata": {},
   "source": [
    "### Prepare the dataset & dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22f70bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2189b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model tokenizer from transformers library\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create \"triage\" dataset class (i.e. data ETL pipeline)\n",
    "# accepts the dataframe and generates tokenized output needed by the model\n",
    "\n",
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data.TITLE[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.ENCODE_CAT[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da95c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset size + dataloader for the neural network\n",
    "\n",
    "# The dataloader loads data into the model in a controlled manner. \n",
    "# This is needed because all the data from the dataset cannot be \n",
    "# loaded into memory at once, hence the amount of data loaded \n",
    "# and then passed to the neural network needs to be controlled.\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset=df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f58571",
   "metadata": {},
   "source": [
    "### Loading the German BERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac2e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the required model from the transformers library\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-german-cased\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e55b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dd8715",
   "metadata": {},
   "source": [
    "### Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3aa5d4",
   "metadata": {},
   "source": [
    "Here we define a training function that trains the model on the training dataset created above, specified number of times (epoch). An epoch defines how many times the complete data will be passed through the network.\n",
    "\n",
    "Following events happen in this function to fine tune the neural network:\n",
    "\n",
    "    The dataloader passes data to the model based on the batch size.\n",
    "    Subsequent output from the model and the actual category are compared to calculate the loss.\n",
    "    Loss value is used to optimize the weights of the neurons in the network.\n",
    "    After every 5000 steps the loss value is printed in the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada091d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aedfdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # when using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d44b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443688de",
   "metadata": {},
   "source": [
    "### Validating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192eee87",
   "metadata": {},
   "source": [
    "During the validation stage we pass the unseen data (test data) to the model. This step determines how good the model performs on the unseen data. In this example, we retained 20% of the data which was separated during dataset above. It may be advisable to use k-fold cross-validation instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee8dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is the validation section to print the accuracy and see how it performs')\n",
    "print('Here we are leveraging the dataloader created for the validation dataset, the approcah is using more of pytorch')\n",
    "\n",
    "acc = valid(model, testing_loader)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874fa13a",
   "metadata": {},
   "source": [
    "### Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdf3444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the files for inference\n",
    "\n",
    "output_model_file = './models/pytorch_germanbert_news.bin'\n",
    "output_vocab_file = './models/vocab_germanbert_news.bin'\n",
    "\n",
    "model_to_save = model\n",
    "torch.save(model_to_save, output_model_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)\n",
    "\n",
    "print('All files saved')\n",
    "print('Stage completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc789d",
   "metadata": {},
   "source": [
    "### Inference using new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbce562",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d119a92d7093c715885be3ec2a460b5a0206e28b05bb37755ae69b94a669be6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
